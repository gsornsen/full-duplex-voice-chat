# ðŸ§­ Realtime Duplex Voice Demo â€” Implementation Milestones & Task Checklists

*Last updated: Oct 2025*
*Linked PRD: `PRD.md`*
*Linked Design Doc: `TDD.md`*

---

## **M0 â€” Repo Scaffold & CI Skeleton**

**Goal:** Establish repo layout, dev tooling, and CI foundation.

### âœ… Tasks

* [x] Initialize repo with base `src/`, `tests/`, `configs/`, `docs/` directories.
* [x] Create `pyproject.toml` using **uv**, with dependencies only (no Python pin).
* [x] Add `ruff`, `mypy`, `pytest`, and `watchfiles` setup.
* [x] Add `justfile` with commands: `lint`, `typecheck`, `test`, `ci`, `run-*`.
* [x] Add `.github/workflows/ci.yml` for lint + type + test.
* [x] Create base `Dockerfile.orchestrator` and `Dockerfile.tts`.
* [x] `docker-compose.yml` for Redis + Orchestrator + TTS skeleton.
* [x] Add placeholder configs (`orchestrator.yaml`, `worker.yaml`).
* [x] Confirm `just ci` passes on empty stubs.

### ðŸ§ª Validation

* [x] `uv lock` works and generates valid environment.
* [x] CI passes in GitHub Actions.
* [x] `just lint`, `just typecheck`, and `just test` succeed.

---

## **M1 â€” gRPC ABI + Mock Worker**

**Goal:** Define and generate TTS streaming interface and control messages.

### âœ… Tasks

* [x] Create `src/rpc/tts.proto`:

  * [x] `Synthesize(stream TextChunk) â†’ stream AudioFrame`
  * [x] `Control`, `ListModels`, `LoadModel`, `UnloadModel`
* [x] Run codegen (`just gen-proto`).
* [x] Implement mock TTS worker with synthetic sine-wave output.
* [x] Implement orchestrator stub to connect to worker and log frames.

### ðŸ§ª Validation

* [x] Integration test: orchestratorâ†’workerâ†’audio bytes verified.
* [x] Unit test: proto roundtrip, `Control` commands.
* [x] Mock worker responds to `PAUSE`, `RESUME`, `STOP`.

---

## **M2 â€” Orchestrator Transport + WS Fallback**

**Goal:** Add WebRTC (browser) and WS (CLI) ingress.

### âœ… Tasks

* [x] Integrate **LiveKit Agent SDK** in a way that can be swapped out with minimal effort for a different/custom orchestrator in the future if necessary.
* [x] Add CLI WS  client for local speech demo.
* [ ] Integrate https://github.com/livekit-examples/agent-starter-react front-end that uses self-hosted livekit infrastructure for local speech web demo
* [x] Implement `VAD` stub and message routing loop.
* [x] Add Redis service registration/discovery skeleton.

### ðŸ§ª Validation

* [ ] WS echo test: textâ†’workerâ†’audioâ†’back.
* [ ] WebRTC echo works locally.
* [ ] Redis keys reflect worker registration.

---

## **M3 â€” Barge-In & State Machine**

**Goal:** Implement pause/resume logic and state transitions.

### âœ… Tasks

* [ ] Add VAD detection using `webrtcvad` (20 ms frames).
* [ ] Implement orchestrator session FSM: `LISTENING â†’ SPEAKING â†’ BARGED_IN`.
* [ ] Send `PAUSE`/`RESUME` to worker on VAD transitions.
* [ ] Add test harness with recorded speech to verify timing.

### ðŸ§ª Validation

* [ ] Barge-in latency p95 < 50 ms (local).
* [ ] Unit tests: FSM transitions.
* [ ] Manual test: voice playback halts on interruption.

---

## **M4 â€” Model Manager v1 (Default/Preload/TTL)**

**Goal:** Introduce model loading, warmup, and eviction system.

### âœ… Tasks

* [ ] Create `model_manager.py` with:

  * [ ] Default model load.
  * [ ] Optional preload list from CLI/config.
  * [ ] Per-model refcounts.
  * [ ] TTL and LRU eviction.
  * [ ] Warmup synth (~300 ms).
* [ ] Expose gRPC methods: `ListModels`, `LoadModel`, `UnloadModel`.
* [ ] Configurable params via YAML + CLI.

### ðŸ§ª Validation

* [ ] Unit tests: load/unload lifecycle.
* [ ] Integration test: dynamic load + TTL eviction.
* [ ] VRAM usage stable under repeated model swaps.

---

## **M5 â€” Piper Adapter (CPU baseline)**

**Goal:** First real TTS adapter; enable end-to-end speechâ†”speech loop.

### âœ… Tasks

* [ ] Implement `adapter_piper.py` conforming to base `TTSAdapter`.
* [ ] Integrate ONNX runtime for CPU inference.
* [ ] Normalize loudness to target RMS.
* [ ] Return 20 ms PCM frames.
* [ ] Add to worker registry.

### ðŸ§ª Validation

* [ ] Speechâ†”speech loop runs on CPU-only machine.
* [ ] Latency p95 < 500 ms.
* [ ] CLI/web demo plays coherent output.

---

## **M6 â€” CosyVoice 2 Adapter (GPU, streaming)**

**Goal:** GPU streaming TTS parity with Piper path.

### âœ… Tasks

* [ ] Implement `adapter_cosyvoice2.py`.
* [ ] Integrate model initialization and streaming inference.
* [ ] Warmup test phrase.
* [ ] Validate pause/resume fidelity.
* [ ] Add unit test for chunk pacing and frame size.

### ðŸ§ª Validation

* [ ] FAL p95 < 300 ms (single GPU).
* [ ] Continuous playback verified under WebRTC.
* [ ] Barge-in stops/resumes smoothly.

---

## **M7 â€” XTTS-v2 Adapter + Voice Cloning**

**Goal:** Add expressive multi-speaker model with reference cloning.

### âœ… Tasks

* [ ] Implement `adapter_xtts_v2.py`.
* [ ] Support reference voice (6â€“10 s sample).
* [ ] Validate streaming mode and chunk pacing.
* [ ] Add metadata in `voicepacks/xtts-v2`.

### ðŸ§ª Validation

* [ ] Quality vs latency benchmark recorded.
* [ ] Clone voice demo runs locally.
* [ ] Frame pacing p95 < 10 ms jitter.

---

## **M8 â€” Sesame / Unsloth Adapter (+LoRA)**

**Goal:** Support LoRA-fine-tuned Sesame model path.

### âœ… Tasks

* [ ] Implement `adapter_sesame.py` (vanilla).
* [ ] Implement `adapter_unsloth_sesame.py` (LoRA variant).
* [ ] Integrate `peft.PeftModel.from_pretrained`.
* [ ] Add CLI `--lora_path` arg for optional adapter.
* [ ] Add to preload list in config.

### ðŸ§ª Validation

* [ ] LoRA swap works at runtime (no restart).
* [ ] Latency p95 < 350 ms (GPU).
* [ ] Model unload verified when idle.

---

## **M9 â€” Routing v1 (Capabilities + Load Balancing)**

**Goal:** Enable orchestrator to intelligently route sessions.

### âœ… Tasks

* [ ] Implement routing policies: least-busy, prefer-resident, random fallback.
* [ ] Redis-based worker heartbeat and capability registry.
* [ ] Add metrics: queue depth, avg latency per worker.

### ðŸ§ª Validation

* [ ] Multiple workers registered, sessions distributed correctly.
* [ ] Load balancing confirmed via logs.
* [ ] Graceful worker death â†’ reroute next request.

---

## **M10 â€” ASR Integration (Whisper small/distil)**

**Goal:** Add real-time transcription path in orchestrator.

### âœ… Tasks

* [ ] Integrate `openai-whisper` small or `whisperx` model.
* [ ] Stream transcriptions to LLM bridge (placeholder).
* [ ] Enable full speechâ†”speech pipeline (ASRâ†’LLMâ†’TTS).

### ðŸ§ª Validation

* [ ] Roundtrip conversation demo works.
* [ ] Whisper CPU/GPU path verified.
* [ ] Logs show ASR partials feeding TTS text stream.

---

## **M11 â€” Observability & Profiling**

**Goal:** Enable measurement and diagnostics.

### âœ… Tasks

* [ ] Add structured logging (JSON).
* [ ] Add Prometheus counters: FAL, jitter, barge-in, RTF.
* [ ] Add `py-spy` targets in `justfile`.
* [ ] Integrate `torch.profiler` NVTX ranges for GPU tracing.

### ðŸ§ª Validation

* [ ] Metrics available via HTTP scrape or logs.
* [ ] py-spy captures valid flamegraph.
* [ ] Nsight Systems/Compute traces export correctly.

---

## **M12 â€” Docker/Compose Smoke & Docs**

**Goal:** Fully reproducible single-GPU demo stack.

### âœ… Tasks

* [ ] Finalize Dockerfiles (CUDA 12.8 base).
* [ ] Ensure `docker compose up --build` spins up redis, tts0, orchestrator.
* [ ] Validate CLI & web client in container network.
* [ ] Add `README.md` setup, `docs/design_v2.1.md`.

### ðŸ§ª Validation

* [ ] Smoke test: local run from clean checkout.
* [ ] End-to-end voice conversation works inside containers.
* [ ] Docs validated for correctness.

---

## **M13 â€” Multi-GPU & Multi-Host Scale-Out**

**Goal:** Expand deployment to N GPUs and hosts via Redis discovery.

### âœ… Tasks

* [ ] Implement per-GPU `CUDA_VISIBLE_DEVICES` orchestration.
* [ ] Extend registry for multi-host IP + port discovery.
* [ ] Add worker auto-registration heartbeat TTL.
* [ ] Validate cross-host gRPC communication.

### ðŸ§ª Validation

* [ ] 2â€“4 GPUs handle simultaneous sessions.
* [ ] Multi-host LAN demo (Redis central) works.
* [ ] No cross-worker model load contention.

---

## **Final Acceptance Gate**

âœ… All milestones integrated and validated:

* [ ] End-to-end speechâ†”speech with barge-in under 50 ms p95.
* [ ] Runtime model switching with TTL unload.
* [ ] Single-GPU + Multi-GPU working demos.
* [ ] CI green; docker compose clean build.
* [ ] Profiling data and metrics available.
* [ ] Docs finalized.

---

**Filename suggestion:** `MILESTONES.md`
You can import each milestone section as a GitHub Issue or Milestone to track progress with checklists.
