name: PR CI

# Comprehensive CI for pull requests to main
# - Runs full test suite in batches to avoid OOM (GitHub Actions runners have 7GB RAM)
# - Batch 1: Unit tests (~907 tests, ~2GB RAM, fast)
# - Batch 2: Integration tests (~208 tests, ~3GB RAM, needs Redis)
# - Batch 3: GPU tests (~20 tests, ~4GB RAM, dedicated runner)
# - Collects code coverage with Codecov
# - Enforces quality gates (lint, typecheck, tests must pass)
# - Status is REQUIRED (blocks merge if failing)
# - Uses aggressive caching for faster runs

on:
  pull_request:
    branches:
      - main
    types: [opened, synchronize, reopened]

env:
  PYTHON_VERSION: "3.13"
  UV_VERSION: "latest"

jobs:
  lint:
    name: Lint (ruff)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --all-extras

      - name: Run ruff
        run: uv run ruff check src/ tests/

  typecheck:
    name: Type Check (mypy)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      # Use composite action for DRY setup
      - name: Setup test environment
        uses: ./.github/actions/setup-test-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          uv-version: ${{ env.UV_VERSION }}

      - name: Run mypy
        run: uv run mypy src/ tests/

  # Batch 1: Unit tests only (fast, no external dependencies, ~2GB RAM)
  test-unit:
    name: Test (Unit - Batch 1/3)
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
      contents: write

    steps:
      - uses: actions/checkout@v4

      # Free up disk space before installation
      - name: Free disk space
        run: |
          echo "Before cleanup:"
          df -h
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/ghc
          sudo rm -rf /opt/hostedtoolcache/CodeQL
          sudo docker image prune --all --force
          echo "After cleanup:"
          df -h

      # Use composite action for DRY setup
      - name: Setup test environment
        uses: ./.github/actions/setup-test-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          uv-version: ${{ env.UV_VERSION }}

      # Clean uv cache after install to save space
      - name: Clean uv cache
        run: |
          uv cache clean
          echo "After cache clean:"
          df -h

      # Run unit tests only (fast, no Docker/Redis needed)
      - name: Run unit tests with coverage
        timeout-minutes: 20
        run: |
          echo "=== Batch 1: Unit Tests (~907 tests) ==="
          uv run pytest tests/unit/ \
            -v \
            -m "not infrastructure and not gpu" \
            --cov=src \
            --cov-report=xml:coverage-unit.xml \
            --cov-report=term \
            --cov-report=html:htmlcov-unit \
            --tb=short \
            --junitxml=test-results-unit.xml \
            -o junit_family=legacy

      # Clean up test artifacts to free space
      - name: Clean test cache
        if: always()
        run: |
          rm -rf .pytest_cache
          rm -rf htmlcov-unit
          echo "After test cleanup:"
          df -h

      - name: Upload unit coverage to Codecov
        if: ${{ !cancelled() }}
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage-unit.xml
          flags: pytest-unit
          name: codecov-pr-ci-unit
          fail_ci_if_error: false
          verbose: true
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload unit test results to Codecov
        if: ${{ !cancelled() }}
        uses: codecov/test-results-action@v1
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: ./test-results-unit.xml
          flags: pytest-unit
          name: codecov-test-results-unit
          fail_ci_if_error: false
          verbose: true

      - name: Upload unit test results as artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-pr-ci-unit
          path: |
            coverage-unit.xml
            test-results-unit.xml
          retention-days: 30

  # Batch 2: Integration tests (needs Redis, ~3GB RAM)
  test-integration:
    name: Test (Integration - Batch 2/3)
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
      contents: write

    # Service containers for integration tests
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v4

      # Free up disk space before installation
      - name: Free disk space
        run: |
          echo "Before cleanup:"
          df -h
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/ghc
          sudo rm -rf /opt/hostedtoolcache/CodeQL
          sudo docker image prune --all --force
          echo "After cleanup:"
          df -h

      # Use composite action for DRY setup
      - name: Setup test environment
        uses: ./.github/actions/setup-test-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          uv-version: ${{ env.UV_VERSION }}

      # Clean uv cache after install to save space
      - name: Clean uv cache
        run: |
          uv cache clean
          echo "After cache clean:"
          df -h

      # Run integration tests (requires Redis)
      - name: Run integration tests with coverage
        timeout-minutes: 25
        run: |
          echo "=== Batch 2: Integration Tests (~208 tests) ==="
          uv run pytest tests/integration/ \
            -v \
            -m "not infrastructure and not gpu" \
            --cov=src \
            --cov-report=xml:coverage-integration.xml \
            --cov-report=term \
            --cov-report=html:htmlcov-integration \
            --tb=short \
            --junitxml=test-results-integration.xml \
            -o junit_family=legacy
        env:
          # Redis is available via service container on localhost:6379
          REDIS_URL: "redis://localhost:6379"

      # Clean up test artifacts to free space
      - name: Clean test cache
        if: always()
        run: |
          rm -rf .pytest_cache
          rm -rf htmlcov-integration
          echo "After test cleanup:"
          df -h

      - name: Upload integration coverage to Codecov
        if: ${{ !cancelled() }}
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage-integration.xml
          flags: pytest-integration
          name: codecov-pr-ci-integration
          fail_ci_if_error: false
          verbose: true
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload integration test results to Codecov
        if: ${{ !cancelled() }}
        uses: codecov/test-results-action@v1
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: ./test-results-integration.xml
          flags: pytest-integration
          name: codecov-test-results-integration
          fail_ci_if_error: false
          verbose: true

      - name: Upload integration test results as artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-pr-ci-integration
          path: |
            coverage-integration.xml
            test-results-integration.xml
          retention-days: 30

  # Batch 3: GPU tests (runs in parallel, dedicated runner, ~4GB RAM)
  gpu-tests:
    name: Test (GPU - Batch 3/3)
    # Use CIRun GPU runner with unique label per run for isolation
    runs-on: "cirun-gpu-8g--${{ github.run_id }}"
    permissions:
      pull-requests: write
      contents: write

    # Service containers for integration tests on GPU runner
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v4

      # Verify GPU availability (sanity check)
      - name: Verify GPU hardware
        run: |
          echo "=== GPU Hardware Check ==="
          nvidia-smi
          echo ""
          echo "GPU: $(nvidia-smi --query-gpu=name --format=csv,noheader)"
          echo "Driver: $(nvidia-smi --query-gpu=driver_version --format=csv,noheader)"
          echo "CUDA: $(nvidia-smi --query-gpu=cuda_version --format=csv,noheader)"
          echo ""
          echo "=== GPU Memory ==="
          nvidia-smi --query-gpu=memory.total,memory.free --format=csv,noheader

      # Use composite action for DRY setup (skip OS deps - pre-installed on GPU runner)
      - name: Setup test environment
        uses: ./.github/actions/setup-test-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          uv-version: ${{ env.UV_VERSION }}
          skip-os-deps: "true"

      # Verify PyTorch CUDA availability
      - name: Verify PyTorch CUDA
        run: |
          uv run python -c "
          import torch
          import sys

          print('=== PyTorch Configuration ===')
          print(f'PyTorch version: {torch.__version__}')
          print(f'CUDA available: {torch.cuda.is_available()}')

          if torch.cuda.is_available():
              print(f'CUDA version: {torch.version.cuda}')
              print(f'cuDNN version: {torch.backends.cudnn.version()}')
              print(f'GPU count: {torch.cuda.device_count()}')
              print(f'GPU name: {torch.cuda.get_device_name(0)}')
              print(f'GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')
          else:
              print('ERROR: CUDA not available in PyTorch!')
              sys.exit(1)
          "


      # Run GPU-specific tests with coverage
      - name: Run GPU tests with coverage
        run: |
          echo "=== Batch 3: GPU Tests (~20 tests) ==="
          uv run pytest tests/ \
            -v \
            -m "gpu" \
            \
            --cov=src \
            --cov-report=xml:coverage-gpu.xml \
            --cov-report=term \
            --cov-report=html:htmlcov-gpu \
            --tb=short \
            --junitxml=test-results-gpu.xml \
            -o junit_family=legacy
        env:
          # Ensure GPU tests run even if CUDA_VISIBLE_DEVICES is set
          CUDA_VISIBLE_DEVICES: "0"
          # Redis is available via service container
          REDIS_URL: "redis://localhost:6379"

      - name: Upload GPU coverage to Codecov
        if: ${{ !cancelled() }}
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage-gpu.xml
          flags: pytest-gpu
          name: codecov-pr-ci-gpu
          fail_ci_if_error: false
          verbose: true
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload GPU test results to Codecov
        if: ${{ !cancelled() }}
        uses: codecov/test-results-action@v1
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: ./test-results-gpu.xml
          flags: pytest-gpu
          name: codecov-test-results-gpu
          fail_ci_if_error: false
          verbose: true

      - name: Upload GPU test results as artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-pr-ci-gpu
          path: |
            coverage-gpu.xml
            test-results-gpu.xml
          retention-days: 30

  build:
    name: Build Check
    runs-on: ubuntu-latest
    needs: [lint, typecheck, test-unit, test-integration]
    # Note: gpu-tests removed from dependencies to avoid blocking on CIRun provisioning
    # GPU tests run in parallel but don't block the build
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Verify uv lock
        run: |
          uv lock --check
          uv sync --frozen

      - name: Build success
        run: |
          echo "## ✅ PR CI Build Successful!" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "All required quality gates passed:" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Linting (ruff)" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Type checking (mypy)" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Unit tests (~907 tests, batch 1/3)" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Integration tests (~208 tests, batch 2/3)" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Code coverage (codecov)" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Build verification (uv lock)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Note: GPU tests (batch 3/3) run in parallel but don't block merge." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**This PR is ready to merge!** 🎉" >> $GITHUB_STEP_SUMMARY

  # Security scanning with Bandit
  security:
    name: Security Scan (bandit)
    runs-on: ubuntu-latest
    continue-on-error: true  # Don't block PR if security scan fails
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install bandit
        run: uv tool install bandit[toml]

      - name: Run bandit
        run: |
          uv tool run bandit -r src/ -f json -o bandit-report.json || true
          uv tool run bandit -r src/ -f txt || true

      - name: Upload security report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: security-report
          path: bandit-report.json
          retention-days: 30

  # Dependency vulnerability scanning
  dependency-check:
    name: Dependency Check
    runs-on: ubuntu-latest
    continue-on-error: true  # Don't block PR if dependency check fails
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --all-extras

      - name: Install pip-audit in project environment
        run: uv pip install pip-audit

      - name: Run pip-audit on project dependencies
        run: |
          uv run pip-audit --desc --output pip-audit-report.json --format json || true
          uv run pip-audit --desc || true

      - name: Upload dependency report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: dependency-report
          path: pip-audit-report.json
          retention-days: 30
