# Full Stack Docker Compose - All Services + Monitoring
#
# Includes: Redis, LiveKit, Caddy, Orchestrator, TTS Workers, Prometheus, Grafana
#
# Usage:
#   docker compose -f docker-compose.full-stack.yml up -d
#   docker compose -f docker-compose.full-stack.yml logs -f
#
# Access Points:
#   - Web UI: https://localhost:8443
#   - Grafana: http://localhost:3033 (admin/admin)
#   - Prometheus: http://localhost:9090
#   - LiveKit: ws://localhost:7880

services:
  # ============================================================================
  # INFRASTRUCTURE
  # ============================================================================

  redis:
    image: redis:7-alpine
    container_name: redis-tts
    restart: unless-stopped
    ports:
      - "6377:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    networks:
      - tts-network
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'

  livekit:
    image: livekit/livekit-server:latest
    container_name: livekit-server
    restart: unless-stopped
    command: --config /etc/livekit/config.yaml
    ports:
      - "7880:7880"      # WebSocket
      - "7881:7881"      # WebRTC TCP
      - "7882:7882/udp"  # TURN/UDP
      - "50000-50099:50000-50099/udp"  # RTC/UDP
    volumes:
      - ./configs/livekit.yaml:/etc/livekit/config.yaml:ro
    environment:
      - LIVEKIT_API_KEY=${LIVEKIT_API_KEY:-devkey}
      - LIVEKIT_API_SECRET=${LIVEKIT_API_SECRET:-devsecret1234567890abcdefghijklmn}
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:7880/"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - tts-network

  caddy:
    image: caddy:2-alpine
    container_name: caddy-proxy
    restart: unless-stopped
    ports:
      - "80:80"
      - "8443:8443"  # HTTPS for web frontend
      - "8444:8444"  # WSS for LiveKit
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile:ro
      - ./voicechat.local+3.pem:/etc/caddy/voicechat.local+3.pem:ro
      - ./voicechat.local+3-key.pem:/etc/caddy/voicechat.local+3-key.pem:ro
      - caddy-data:/data
      - caddy-config:/config
      - caddy-logs:/var/log/caddy
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:2019/config/"]
      interval: 10s
      timeout: 5s
      retries: 3
    depends_on:
      web:
        condition: service_healthy
      livekit:
        condition: service_healthy
    networks:
      - tts-network
      - docker-host-network

  # ============================================================================
  # WEB FRONTEND
  # ============================================================================

  web:
    build:
      context: .
      dockerfile: Dockerfile.web
    image: full-duplex-voice-chat-web:latest
    container_name: web-frontend
    restart: unless-stopped
    environment:
      # Client-side (browser) environment variables
      - NEXT_PUBLIC_LIVEKIT_URL=wss://voicechat.local:8444
      - NEXT_PUBLIC_LIVEKIT_API_KEY=${LIVEKIT_API_KEY:-devkey}
      # Server-side (API routes) environment variables
      - LIVEKIT_URL=ws://livekit:7880
      - LIVEKIT_API_KEY=${LIVEKIT_API_KEY:-devkey}
      - LIVEKIT_API_SECRET=${LIVEKIT_API_SECRET:-devsecret1234567890abcdefghijklmn}
      - NODE_ENV=production
    ports:
      - "3000:3000"
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3000/', (r) => process.exit(r.statusCode === 200 ? 0 : 1))"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - tts-network
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '1.0'

  # ============================================================================
  # ORCHESTRATOR
  # ============================================================================

  orchestrator:
    build:
      context: .
      dockerfile: Dockerfile.orchestrator
    image: full-duplex-voice-chat-orchestrator:latest
    container_name: orchestrator
    restart: on-failure:5
    entrypoint: ["/app/docker/entrypoint-orchestrator.sh"]
    stop_signal: SIGTERM
    stop_grace_period: 90s
    environment:
      - ORCHESTRATOR_MODE=${ORCHESTRATOR_MODE:-agent}
      - REDIS_URL=redis://redis:6379
      - LIVEKIT_URL=ws://livekit:7880
      - LIVEKIT_API_KEY=${LIVEKIT_API_KEY:-devkey}
      - LIVEKIT_API_SECRET=${LIVEKIT_API_SECRET:-devsecret1234567890abcdefghijklmn}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}  # Required for agent mode (reads from .env/.env.local)
      - DEFAULT_TTS_WORKER=${DEFAULT_TTS_WORKER:-tts-cosyvoice:7001}
      - TTS_WORKER_ADDRESS=${TTS_WORKER_ADDRESS:-tts-cosyvoice:7001}  # gRPC worker address for agent mode
      - ROUTING_STRATEGY=${ROUTING_STRATEGY:-least_loaded}
      - ROUTING_AFFINITY_ENABLED=${ROUTING_AFFINITY_ENABLED:-true}
      - ASR_ENABLED=${ASR_ENABLED:-true}
      # - ASR_MODEL=${ASR_MODEL:-whisper-base}
      # - ASR_MODEL=${ASR_MODEL:-whisperx}
      - ASR_ADAPTER=${ASR_ADAPTER:-whisperx}
      - ASR_DEVICE=cuda  # Force CUDA (override .env file)
      - ASR_MODEL_SIZE=${ASR_MODEL_SIZE:-small}
      - ASR_LANGUAGE=${ASR_LANGUAGE:-en}
      - ASR_COMPUTE_TYPE=float16  # Fastest GPU inference (half precision)
      - METRICS_ENABLED=true
      - METRICS_PORT=8081
      - DEFAULT_MODEL=cosyvoice2-en-base  # Explicit model for fastest startup
      - DEFAULT_MODEL_ID=cosyvoice2-en-base  # Explicit model ID
      # - ADAPTER_TYPE=${ADAPTER_TYPE:-cosyvoice2}
      - PARALLEL_SYNTHESIS_ENABLED=${PARALLEL_SYNTHESIS_ENABLED:-true}
      - PARALLEL_SYNTHESIS_NUM_WORKERS=${PARALLEL_SYNTHESIS_NUM_WORKERS:-2}  # 2 workers to reduce GPU contention (3 may cause contention without TensorRT fully optimized)
      - PARALLEL_SYNTHESIS_MAX_QUEUE_DEPTH=${PARALLEL_SYNTHESIS_MAX_QUEUE_DEPTH:-20}  # Higher queue depth to buffer more sentences for true parallelism
      - PARALLEL_SYNTHESIS_GPU_LIMIT=${PARALLEL_SYNTHESIS_GPU_LIMIT:-2}  # Max concurrent GPU ops (matches worker count to reduce contention)
      - PARALLEL_SYNTHESIS_PREFETCH_ENABLED=${PARALLEL_SYNTHESIS_PREFETCH_ENABLED:-true}  # Enable prefetching for continuous streaming (experimental)
      - PARALLEL_SYNTHESIS_PREFETCH_DEPTH=${PARALLEL_SYNTHESIS_PREFETCH_DEPTH:-3}  # Max sentences to prefetch ahead
      - ENABLE_CONTINUATION_DETECTION=${ENABLE_CONTINUATION_DETECTION:-true}
      # - DUAL_LLM_ENABLED=${DUAL_LLM_ENABLED:-false}
      - DUAL_LLM_ENABLED=false
      # Fix uv cache permissions
      - UV_CACHE_DIR=/tmp/.uv-cache
      # GPU configuration for WhisperX ASR
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    ports:
      - "8080:8080"  # WebSocket
      - "8081:8081"  # Metrics/Health
    volumes:
      - ./src:/app/src:ro
      - ./docker/entrypoint-orchestrator.sh:/app/docker/entrypoint-orchestrator.sh:ro
      # Persistent caches to avoid re-downloading models/files on restart
      - huggingface-cache:/home/orchestrator/.cache/huggingface:rw
      - torch-cache:/home/orchestrator/.cache/torch:rw
      - mfa-cache:/home/orchestrator/.cache/mfa:rw
      - nemo-cache:/home/orchestrator/.cache/nemo:rw
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s
    depends_on:
      redis:
        condition: service_healthy
      livekit:
        condition: service_healthy
      tts-cosyvoice:
        condition: service_healthy
    networks:
      - tts-network
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # ============================================================================
  # TTS WORKERS
  # ============================================================================

  # Piper TTS Worker (CPU - optional, uncomment to use)
  # tts0:
  #   build:
  #     context: .
  #     dockerfile: Dockerfile.tts
  #   image: full-duplex-voice-chat-tts-worker:latest
  #   container_name: tts-worker-0
  #   restart: unless-stopped
  #   environment:
  #     - REDIS_URL=redis://redis:6379
  #     - WORKER_NAME=tts-worker-0
  #     - WORKER_PORT=7001
  #     - ADAPTER_TYPE=piper
  #     - DEFAULT_MODEL=piper-en-us-lessac-medium
  #     - METRICS_ENABLED=true
  #     - METRICS_PORT=9090
  #   ports:
  #     - "7001:7001"  # gRPC
  #     - "9090:9090"  # Metrics
  #   volumes:
  #     - ./src:/app/src:ro
  #     - ./voicepacks:/app/voicepacks:ro
  #   healthcheck:
  #     test: ["CMD", "timeout", "1", "bash", "-c", "</dev/tcp/localhost/7001"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 60s
  #   depends_on:
  #     redis:
  #       condition: service_healthy
  #   networks:
  #     - tts-network
  #     - docker-host-network
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 4G
  #         cpus: '2.0'

  # CosyVoice TTS Worker (GPU)
  tts-cosyvoice:
    build:
      context: .
      dockerfile: Dockerfile.tts-cosyvoice
    image: full-duplex-voice-chat-tts-cosyvoice:latest
    container_name: tts-cosyvoice
    restart: on-failure:5
    runtime: nvidia
    environment:
      - REDIS_URL=redis://redis:6379
      - WORKER_NAME=tts-cosyvoice
      - WORKER_PORT=7001
      - ADAPTER_TYPE=cosyvoice2
      - DEFAULT_MODEL=cosyvoice2-en-base
      - DEFAULT_MODEL_ID=cosyvoice2-en-base
      - TTL_MS=600000
      - RESIDENT_CAP=2
      - LOG_LEVEL=INFO
      - GRPC_TRACE=""
      - GRPC_VERBOSITY=ERROR
      - PYTHONWARNINGS=ignore::FutureWarning,ignore::UserWarning
      - CUDA_VISIBLE_DEVICES=0
      - METRICS_ENABLED=true
      - METRICS_PORT=9091
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      # CUDA optimizations for better performance
      - CUDA_LAUNCH_BLOCKING=0  # Allow asynchronous kernel execution
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512  # Optimize memory fragmentation
      - SAMPLE_RATE=24000
      - FRAME_MS=20
      - CHANNELS=1
      - PRELOAD_MODEL_IDS=cosyvoice2-en-base
      - TTL_MS=600000
      - MIN_RESIDENCY_MS=120000
      - EVICT_CHECK_INTERVAL_MS=30000
      - RESIDENT_CAP=2
      - MAX_PARALLEL_LOADS=2
      - WARMUP_ENABLED=true
      - WARMUP_TEXT="Testing warmup synthesis for model initialization."
      - ENABLE_EMOTION_CONTROL=true
      - DEFAULT_EMOTION=neutral
      - USE_FLASH_ATTENTION=true
      - USE_TORCH_COMPILE=true
      - USE_FP16=true
      - USE_BF16=false
      - USE_TENSORRT=true  # TensorRT optimization for ~2x speedup on RTX 4090 (requires initial compilation)
      - USE_VLLM=false  # vLLM acceleration for LLM stage (~2x speedup on LLM, ~20-30% overall speedup)
      - ENABLE_TEXT_NORMALIZATION=true
      - TEXT_NORMALIZATION_LANG=en
      - LOG_LEVEL=INFO
      - ENABLE_PROFILING=false
      - ENABLE_METRICS=true
      - METRICS_PORT=9091
      - INFERENCE_BATCH_SIZE=1
      # - MAX_TEXT_LENGTH=500
      - WORKER_THREADS=4
      - MAX_CONCURRENT_SESSIONS=8
      - QUEUE_SIZE=32
      - QUEUE_TIMEOUT_MS=5000
      - ESTIMATED_VRAM_GB=6
      - LOG_LEVEL=INFO
      - LOG_FORMAT=json
      - ENABLE_STRUCTURED_LOGS=true
      - AUDIO_BUFFER_SIZE_FRAMES=10
      - AUDIO_RESAMPLING_QUALITY=high
      - MODEL_LOAD_TIMEOUT_MS=60000
      - MODEL_UNLOAD_TIMEOUT_MS=10000
    ports:
      - "7002:7001"  # gRPC (avoid conflict if tts0 is also running)
      - "9091:9091"  # Metrics
    volumes:
      - ./src:/app/src:ro
      - cosyvoice-voicepacks:/app/voicepacks:rw
      # Cache directories - use /home/cosyvoice/.cache (not /root) since container runs as cosyvoice user (uid 1000)
      - huggingface-cache:/home/cosyvoice/.cache/huggingface:rw
      - torch-cache:/home/cosyvoice/.cache/torch:rw
      - modelscope-cache:/home/cosyvoice/.cache/modelscope:rw
      - mfa-cache:/home/cosyvoice/.cache/mfa:rw
    healthcheck:
      test: ["CMD", "timeout", "1", "bash", "-c", "</dev/tcp/localhost/7001"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 900s
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - tts-network
      - docker-host-network
    deploy:
      resources:
        limits:
          memory: 16G
          cpus: '4.0'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # ============================================================================
  # MONITORING STACK
  # ============================================================================

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus-tts
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--storage.tsdb.retention.time=30d'
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 10s
    networks:
      - tts-network
      - monitoring
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'

  grafana:
    image: grafana/grafana:latest
    container_name: grafana-tts
    restart: unless-stopped
    ports:
      - "3033:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=http://localhost:3033
      - GF_INSTALL_PLUGINS=
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    depends_on:
      prometheus:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 30s
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'

networks:
  tts-network:
    driver: bridge
  monitoring:
    driver: bridge
  docker-host-network:
    driver: bridge

volumes:
  caddy-data:
    driver: local
  caddy-config:
    driver: local
  caddy-logs:
    driver: local
  huggingface-cache:
    driver: local
  torch-cache:
    driver: local
  modelscope-cache:
    driver: local
  mfa-cache:
    driver: local
  nemo-cache:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local
  cosyvoice-voicepacks:
    driver: local
