# CIRun Configuration for GPU CI
# Provides on-demand GPU runners for testing GPU-accelerated features
# Documentation: https://docs.cirun.io/reference/configuration

version: "1.0"

# Dual-region GPU runner configuration
# Primary: g5g.xlarge (eu-south-2) - cheapest at ~$0.17/hr spot
# Fallback: g4dn.xlarge (us-west-2) - x86 compatibility at ~$0.23/hr spot
runners:
  # PRIMARY: g5g.xlarge in eu-south-2 (Milan, Italy)
  # - NVIDIA T4G GPU (16 GB VRAM) on ARM Graviton2
  # - Cheapest spot option ~$0.17/hr
  # - Good for TTS inference (PyTorch ARM support)
  - name: "gpu-tests-eu"
    cloud: "aws"
    region: "eu-south-2"
    instance_type: "g5g.xlarge"

    # GPU configuration
    gpu: "nvidia-tesla-t4g"

    # Use spot instances for cost savings
    preemptible: true

    # Labels for GitHub Actions runner selection
    labels:
      - "cirun-gpu-g5g-eu"
      - "cirun-gpu-8g"  # Generic label (primary matches first)

    # Runner timeout: 30 minutes
    timeout: 30

    # Disk size: 100 GB
    disk_size: 100

    # Auto-shutdown after 5 minutes of inactivity
    idle_timeout: 5

    # Machine image: Deep Learning AMI with NVIDIA drivers
    machine_image: "ami-ubuntu-22.04-nvidia-cuda-12-1"

    # Environment setup (executed on runner startup)
    setup:
      - name: "Install system dependencies"
        run: |
          sudo apt-get update
          sudo apt-get install -y libportaudio2 curl redis-tools

      - name: "Verify GPU availability"
        run: |
          nvidia-smi
          echo "GPU detected: $(nvidia-smi --query-gpu=name --format=csv,noheader)"

      - name: "Verify CUDA availability"
        run: |
          nvcc --version || echo "CUDA toolkit not in PATH (may be available via PyTorch)"

  # FALLBACK: g4dn.xlarge in us-west-2 (Oregon)
  # - NVIDIA T4 GPU (16 GB VRAM) on x86-64
  # - More expensive but better x86 compatibility
  # - Spot price ~$0.23/hr
  - name: "gpu-tests-us"
    cloud: "aws"
    region: "us-west-2"
    instance_type: "g4dn.xlarge"

    # GPU configuration
    gpu: "nvidia-tesla-t4"

    # Use spot instances for cost savings
    preemptible: true

    # Labels for GitHub Actions runner selection
    labels:
      - "cirun-gpu-g4dn-us"
      - "cirun-gpu-8g-fallback"

    # Runner timeout: 30 minutes
    timeout: 30

    # Disk size: 100 GB
    disk_size: 100

    # Auto-shutdown after 5 minutes of inactivity
    idle_timeout: 5

    # Machine image: Deep Learning AMI with NVIDIA drivers
    machine_image: "ami-ubuntu-22.04-nvidia-cuda-12-1"

    # Environment setup
    setup:
      - name: "Install system dependencies"
        run: |
          sudo apt-get update
          sudo apt-get install -y libportaudio2 curl redis-tools

      - name: "Verify GPU availability"
        run: |
          nvidia-smi
          echo "GPU detected: $(nvidia-smi --query-gpu=name --format=csv,noheader)"

      - name: "Verify CUDA availability"
        run: |
          nvcc --version || echo "CUDA toolkit not in PATH (may be available via PyTorch)"

# Resource limits
# Maximum concurrent GPU runners (prevents cost overruns)
max_runners: 2

# Auto-cleanup: terminate runners after job completion
auto_cleanup: true

# Monitoring and logging
monitoring:
  enabled: true
  # Send metrics to CIRun dashboard
  metrics:
    - "gpu_utilization"
    - "gpu_memory_used"
    - "cpu_utilization"
    - "disk_usage"
