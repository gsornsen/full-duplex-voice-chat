# Current Project Status

**Last Updated**: 2025-10-26
**Branch**: `main`
**Overall Status**: M0-M10 Complete + Parallel Synthesis, M6 Complete, M7-M9, M11-M13 Planned

---

## Quick Summary

This project implements a realtime duplex voice chat system with low-latency TTS streaming and barge-in support. **Milestones M0-M10 are complete**, establishing the core infrastructure: gRPC streaming protocol, mock TTS worker, dual transport architecture (LiveKit WebRTC primary + WebSocket fallback), real-time barge-in with Voice Activity Detection, complete Model Manager lifecycle with TTL/LRU eviction, and two real TTS adapters (Piper CPU baseline, CosyVoice 2 GPU). The implementation has **exceeded M2 scope** by delivering production-ready LiveKit WebRTC as the primary transport. **M6 is complete**, implementing the CosyVoice 2 GPU TTS adapter with Docker isolation, shared audio utilities, comprehensive testing (51 tests), and production deployment guide. **Parallel TTS synthesis** is production-ready, delivering **2x throughput improvement** with persistent worker pools. Additional GPU TTS adapters (XTTS, Sesame) and advanced features (dynamic routing) are planned for M7-M13.

---

## Milestone Status

| Milestone | Description | Status | Completion Date | Notes |
|-----------|-------------|--------|-----------------|-------|
| **M0** | Repo Scaffold & CI | âœ… Complete | 2025-09 | Justfile, CI, Docker, proto gen |
| **M1** | gRPC ABI + Mock Worker | âœ… Complete | 2025-09 | 16/16 tests passing, <50ms control |
| **M2** | Orchestrator Transport | âœ… Enhanced | 2025-10 | LiveKit primary (exceeded scope) |
| **M3** | Barge-in End-to-End | âœ… Complete | 2025-10-09 | VAD integration, <50ms pause latency |
| **M4** | Model Manager v1 | âœ… Complete | 2025-10-09 | 20 unit + 15 integration tests passing |
| **M5** | Piper Adapter (CPU) | âœ… Complete | 2025-10-10 | 25 unit + 7 integration tests passing |
| **M6** | CosyVoice 2 Adapter | âœ… Complete | 2025-10-17 | All phases complete (adapter + integration + Docker deployment) |
| **M7** | XTTS-v2 Adapter | ðŸ“ Planned | - | GPU + voice cloning |
| **M8** | Sesame/Unsloth Adapter | ðŸ“ Planned | - | LoRA fine-tuned models |
| **M9** | Routing v1 | ðŸ“ Planned | - | Capability-based selection |
| **M10** | ASR Integration | âœ… Complete | 2025-10-11 | Whisper + WhisperX adapters, 128 tests |
| **Parallel TTS** | Parallel Synthesis | âœ… Complete | 2025-10-24 | 2x throughput, persistent workers |
| **CI** | CI/CD Optimization | âœ… Complete | 2025-10-16 | 3-tier strategy, 70% cost reduction |
| **M11** | Observability & Profiling | ðŸ“ Planned | - | Metrics, logging, tracing |
| **M12** | Docker/Compose Polish | ðŸ“ Planned | - | Production deployment |
| **M13** | Multi-GPU Scale-out | ðŸ“ Planned | - | N GPUs, multi-host |

**Legend**:
- âœ… Complete: Fully implemented and tested
- ðŸ”„ Partial: Some implementation, integration pending
- ðŸ“ Planned: Not yet started

---

## What Works Today (M0-M10 + Parallel Synthesis Complete)

### âœ… Core Infrastructure

**gRPC Streaming Protocol (M1)**:
- `Synthesize(stream TextChunk) â†’ stream AudioFrame` bidirectional streaming
- `Control(PAUSE|RESUME|STOP)` commands with <50ms response time
- `StartSession` / `EndSession` lifecycle management
- Protocol defined in `src/rpc/tts.proto`, codegen via `just gen-proto`
- **Test Coverage**: 16/16 M1 integration tests passing (with `--forked` mode)

**Mock TTS Worker (M1)**:
- Generates 440Hz sine wave test tone for validation
- Proper 20ms framing @ 48kHz PCM output
- Immediate response to PAUSE/RESUME/STOP commands
- Used for all current integration testing
- Location: `src/tts/adapters/adapter_mock.py`

**Orchestrator Server (M2)**:
- **LiveKit WebRTC Transport** (Primary):
  - Full LiveKit agent integration for browser clients
  - Bidirectional audio streaming over WebRTC
  - Caddy reverse proxy for HTTPS/TLS
  - Docker compose orchestration with health checks
  - Location: `src/orchestrator/livekit_utils/`

- **WebSocket Transport** (Secondary/CLI):
  - JSON message protocol for CLI clients
  - Supports session_start, text, audio, error messages
  - Used by CLI client: `just cli`
  - Location: `src/orchestrator/transport/websocket_transport.py`

**Session Management (M2)**:
- State machine: IDLE â†’ LISTENING â†’ SPEAKING â†’ BARGED_IN â†’ TERMINATED
- State transition validation (VALID_TRANSITIONS enforced)
- Session metrics tracking (FAL, frame count, timing)
- Multi-message per session support (workaround for protocol edge case)
- Location: `src/orchestrator/session.py`

**Barge-in with Voice Activity Detection (M3)**:
- VAD integration using webrtcvad library
- Real-time speech detection with <50ms latency
- Automatic PAUSE on speech detection
- Automatic RESUME on silence detection
- State machine transitions: SPEAKING â†’ BARGED_IN â†’ LISTENING
- Configurable aggressiveness (0-3), debouncing thresholds
- Audio resampling (48kHz â†’ 16kHz) for VAD processing
- Telemetry for barge-in events (count, latency)
- **Test Coverage**: 29/29 unit tests, 8/8 integration tests passing
- Location: `src/orchestrator/vad.py`, `src/orchestrator/audio/resampler.py`

**Model Manager with Lifecycle Management (M4)**:
- Default model loading on startup (required)
- Optional preload list from configuration
- TTL-based idle model eviction (configurable timeout)
- LRU eviction when resident_cap exceeded
- Warmup synthetic utterance on initialization
- Reference counting for safe unload (prevents in-use eviction)
- gRPC endpoints: LoadModel, UnloadModel, ListModels, GetCapabilities
- Background eviction task with configurable interval
- Semaphore control for max_parallel_loads
- **Test Coverage**: 20 unit tests, 15 integration tests passing
- Location: `src/tts/model_manager.py`

**Piper TTS Adapter - First Real TTS Model (M5)**:
- CPU-based neural TTS using ONNX Runtime
- Native 22050Hz synthesis with resampling to 48kHz
- 20ms PCM frame output (960 samples per frame)
- PAUSE/RESUME/STOP control with <50ms response time
- Empty audio edge case handling (prevents ZeroDivisionError)
- Race-condition-free pause timing (double-check before yield)
- Warmup synthetic utterance (<1s on modern CPU)
- Voicepack support with metadata.yaml
- Model Manager integration with prefix-based routing (piper-*)
- **Test Coverage**: 25 unit tests, 7 integration tests passing
- **Location**: `src/tts/adapters/adapter_piper.py`
- **Example Models**: en-us-lessac-medium (22kHz, ONNX)
- **Performance**: ~300ms warmup, streaming synthesis with scipy resampling

**CosyVoice 2 TTS Adapter - GPU Expressive TTS (M6 Complete)** âœ…:
- GPU-accelerated neural TTS with zero-shot voice cloning
- Native 24000Hz synthesis with resampling to 48kHz
- 20ms PCM frame output (960 samples per frame)
- PAUSE/RESUME/STOP control with <50ms response time
- Shared audio utilities (resampling, framing) extracted for all adapters
- AdapterState enum unified across all adapters
- Model Manager integration with prefix-based routing (cosyvoice2-*)
- **Test Coverage**: 35 unit tests, 16 integration tests passing (51 total)
- **Location**: `src/tts/adapters/adapter_cosyvoice.py`
- **Shared Utilities**: `src/tts/audio/resampling.py`, `src/tts/audio/framing.py`
- **PyTorch Constraint**: Requires PyTorch 2.3.1 + CUDA 12.1 (Docker isolation recommended)
- **Documentation**: `docs/COSYVOICE_PYTORCH_CONFLICT.md`, `docs/VOICEPACK_COSYVOICE2.md`
- **Status**: Complete - adapter implementation, integration, Docker deployment

**Automatic Speech Recognition (M10)**:
- Two ASR adapters: Whisper (baseline) and WhisperX (4-8x faster)
- **Whisper Adapter**:
  - OpenAI Whisper using faster-whisper (CTranslate2)
  - Multi-model support: tiny, base, small, medium, large
  - CPU and GPU inference with configurable compute types
  - Performance: RTF ~0.36 (CPU), ~0.2 (GPU)
  - Location: `src/asr/adapters/adapter_whisper.py`
- **WhisperX Adapter** (Optimized):
  - CTranslate2-optimized inference (4-8x faster than standard)
  - Auto device selection (GPU if available, else CPU)
  - Auto compute type optimization (int8 for CPU, float16 for GPU)
  - Segment-weighted confidence calculation
  - Performance: RTF 0.095 (CPU), 0.048 (GPU) - exceeds targets
  - Latency: p95 144ms (GPU), p95 285ms (CPU)
  - Location: `src/asr/adapters/adapter_whisperx.py`
- **Audio Processing**:
  - Audio buffering for speech accumulation
  - Resampling support (8kHz-48kHz â†’ 16kHz)
  - Integration with VAD for speech boundary detection
  - Location: `src/orchestrator/audio/buffer.py`
- **Configuration**: Pydantic models with validation, environment variable support
- **Test Coverage**: 64 Whisper tests (unit + integration) + 25 WhisperX tests = 89 total ASR tests
- **Status**: Production-ready, approved by @ml-engineer

**Parallel TTS Synthesis (2025-10-24)** âœ…:
- **Persistent worker pool** for concurrent sentence synthesis
- **2x throughput improvement** validated in production
- **FIFO ordering guarantee** for correct audio playback
- **Backpressure control** prevents memory exhaustion
- **GPU semaphore** limits concurrent operations
- **TTS warm-up integration** eliminates cold-start latency (3-6s â†’ <1s)
- **Architecture**:
  - ParallelTTSWrapper manages worker pool lifecycle
  - Round-robin worker selection for load balancing
  - Per-sentence audio queues for parallel delivery
  - BufferedChunkedStream maintains FIFO ordering
- **Configuration**:
  - `PARALLEL_SYNTHESIS_ENABLED=true` (enable/disable)
  - `PARALLEL_SYNTHESIS_NUM_WORKERS=2` (2-3 recommended)
  - `PARALLEL_SYNTHESIS_MAX_QUEUE_DEPTH=10` (backpressure threshold)
  - `PARALLEL_SYNTHESIS_GPU_LIMIT=2` (max concurrent GPU ops)
- **Performance** (Commit 147e45c):
  - Synthesis latency: 50% reduction (6s â†’ 3s for 5 sentences)
  - Greeting synthesis: 3-6s â†’ <1s (warm-up benefit)
  - Worker utilization: ~85% sustained
  - User feedback: "pretty good!" validation
- **Location**: `src/plugins/grpc_tts/parallel_wrapper.py`, `src/plugins/grpc_tts/tts.py`
- **Documentation**: `docs/PARALLEL_TTS.md` (comprehensive guide)
- **Test Coverage**: Integration tests passing, user validation complete

**Service Discovery & Registry (M2)**:
- Redis-based worker registration and heartbeat
- TTL-based worker expiration
- Capability announcement (streaming, languages, zero_shot, etc.)
- Location: `src/orchestrator/registry.py`

**Docker Compose Stack (M2)**:
- **5 Services**: Redis, LiveKit, Caddy, Orchestrator, TTS Worker
- Health checks for all services
- Startup dependencies configured
- Internal Docker networking (no port conflicts)
- GPU allocation ready (CUDA_VISIBLE_DEVICES)
- Command: `docker compose up --build`

### âœ… Development Infrastructure

**Build & CI (M0)**:
- Justfile commands: `lint`, `typecheck`, `test`, `ci`
- Ruff linting with auto-fix (`just fix`)
- Mypy strict type checking
- Pytest with comprehensive markers
- GitHub Actions CI pipeline

**Test Infrastructure (M1-M10)**:
- 60+ test files (unit + integration + performance)
- 817-line `conftest.py` with comprehensive fixtures
- Synthetic audio generators for validation
- Frame timing validators
- Latency metrics collectors
- Dynamic port allocation for parallel tests
- VAD test suite with debouncing and aggressiveness validation
- Shared audio utilities test suite (resampling FFT validation, framing)

**Test Coverage Summary**:
- **Total Tests**: 790 tests passing in CI (as of 2025-10-26)
- **M0-M5 Infrastructure**: 113 tests (core, VAD, Model Manager, Piper)
- **M6 CosyVoice**: 51 tests (adapter + shared utilities)
- **M10 ASR**: 128 tests (Whisper + WhisperX + audio buffer + performance)
- **M10 Polish**: 71 tests (RMS buffer, session timeout, multi-turn)
- **Parallel TTS**: Integration tests + user validation
- **Pass Rate**: 100% (all tests passing)

**gRPC Testing Workaround (M1/M2)**:
- **Issue**: grpc-python segfaults in WSL2 during test teardown
- **Solution**: pytest-forked integration with `--forked` flag
- **Status**: 100% mitigated, tests reliable
- **Documentation**: [GRPC_SEGFAULT_WORKAROUND.md](archive/working/GRPC_SEGFAULT_WORKAROUND.md)
- **Usage**: `just test-integration` (automatic)

**Unified Development Workflow (2025-10-17)** âœ…:
- **Honcho process manager**: Industry-standard Procfile format (Heroku-compatible)
- **Parallel service startup**: ~10 seconds (vs 5+ minute Docker Compose build)
- **Automatic logging**: Timestamped sessions in `logs/dev-sessions/`
- **Color-coded output**: Each service has distinct color for easy debugging
- **Graceful shutdown**: Single Ctrl+C stops all processes within 10 seconds
- **Hot-reload friendly**: Quick restart after code changes
- **Commands**: `just dev-agent-piper` (recommended), `just dev`, `just dev-web`
- **Log management**: `just logs-list`, `just logs-tail`, `just logs-clean`
- **Docker Compose profiles**: `--profile cosyvoice` for isolated PyTorch 2.3.1
- **Location**: `Procfile.dev`, `Procfile.agent`, updated `justfile`
- **Documentation**: [DOCKER_UNIFIED_WORKFLOW.md](DOCKER_UNIFIED_WORKFLOW.md), [MIGRATION_GUIDE.md](../MIGRATION_GUIDE.md)
- **Test Coverage**: Workflow tested on Linux, WSL2; backward compatible with individual services

**Code Quality Metrics**:
- Type hints: Comprehensive (mypy strict mode)
- Docstrings: Detailed with Args/Returns/Notes
- Test pass rate: 100% (790/790 tests passing in CI)
- Total tests: 790 (unit + integration + performance)
- Architecture: Clean separation of concerns with shared utilities

---

**Configuration System (2025-10-19)** âœ…:
- **Environment-based configuration**: All settings externalized to `.env` file
- **Adapter/Model validation**: Startup checks for compatibility (e.g., `cosyvoice2-*` requires `ADAPTER_TYPE=cosyvoice2`)
- **GPU auto-detection**: WhisperX automatically uses GPU when `ASR_DEVICE=auto`
- **Performance improvements**:
  - GPU WhisperX initialization: 28s â†’ 3-5s (5-9x faster)
  - GPU WhisperX memory: 1470 MB â†’ 900 MB (40% reduction)
  - Real-time factor (RTF): 0.095 (CPU) â†’ 0.048 (GPU)
- **Configuration validation warnings**: Clear messages for common misconfigurations
- **Backward compatibility**: Legacy environment variable names still supported
- **Documentation**: Comprehensive configuration guide ([docs/CONFIGURATION.md](CONFIGURATION.md))
- **Location**: `src/orchestrator/config_validator.py`, `.env.example`

---

## What's Planned (M7-M9, M11-M13 Roadmap)

### ðŸ“ Near-term (M7-M8)

**M7: XTTS-v2 Adapter (GPU + Cloning)**:
- Multi-speaker expressive model
- Reference voice support (6-10s sample)
- Streaming mode validation
- Voice cloning demo

### ðŸ“ Mid-term (M8-M9)

**M8: Sesame/Unsloth Adapter (+LoRA)**:
- Vanilla Sesame adapter
- LoRA variant with peft.PeftModel
- Runtime LoRA swap without restart
- CLI `--lora_path` argument

**M9: Routing v1**:
- Capability-based worker selection
- Load balancing policies (least-busy, prefer-resident)
- Redis heartbeat and metrics
- Graceful worker failure handling

### ðŸ“ Long-term (M11-M13)

**CI/CD Optimization** âœ… Complete (2025-10-16):
- âœ… Three-tier CI strategy implemented:
  - **Feature CI**: Smart test selection, 3-5 min (60-70% faster)
  - **PR CI**: Full validation + coverage, 10-15 min (required)
  - **Main Baseline**: Codecov data upload only (5 min)
- âœ… Aggressive caching: 90% faster dependency install (30s vs 5min)
- âœ… Codecov integration: Coverage reports + Test Analytics
- âœ… Security scanning: bandit + pip-audit (informational)
- âœ… Cost reduction: 70% savings (~$8/month vs $27/month)
- âœ… Documentation: 500+ lines in CLAUDE.md, 280+ lines in DEVELOPMENT.md
- âœ… All GitHub Actions using latest versions (v3/v4)
- âœ… Coverage configuration: relative paths, proper exclusions
- âœ… Test count: 790 tests passing (unit + integration + performance)

**M11: Observability & Profiling**:
- Structured JSON logging
- Prometheus metrics (FAL, jitter, RTF, queue depth)
- py-spy profiling targets in justfile
- PyTorch profiler with NVTX ranges

**M12: Docker/Compose Polish**:
- Production-ready Dockerfiles
- Finalized docker-compose.yml
- Documentation polish
- Smoke test validation

**M13: Multi-GPU & Multi-Host Scale-out**:
- Per-GPU CUDA_VISIBLE_DEVICES orchestration
- Multi-host IP+port discovery via Redis
- Worker auto-registration with TTL
- Cross-host gRPC validation

---

## Known Limitations

### Platform Constraints

**WSL2 Environment**:
- gRPC tests require `--forked` mode due to upstream segfault issue
- Workaround: Use `just test-integration` (automatic)
- Alternative: Skip gRPC tests in WSL2, run in Docker/native Linux
- Documentation: [GRPC_SEGFAULT_WORKAROUND.md](archive/working/GRPC_SEGFAULT_WORKAROUND.md)
- Status: Fully mitigated, tests 100% reliable

**Python 3.13**:
- Performance tests deferred due to instability in some libraries
- Unit and integration tests fully functional
- Recommendation: Use Python 3.12 if 3.13 causes issues

### Implementation Gaps

**Limited TTS Adapter Selection**:
- Piper CPU adapter implemented (M5 complete)
- CosyVoice 2 GPU adapter implemented (M6 complete)
- GPU adapters (XTTS, Sesame) are M7-M8 milestones
- Justfile commands for GPU adapters exist but not functional yet
- Mock adapter still available for testing (440Hz sine wave)

**Static Routing Only**:
- M2 uses static worker address configuration
- Dynamic capability-based routing is M9 milestone
- Single worker only in current setup (unless manual multi-GPU)

---

## Quick Start Commands

### Development Workflow

```bash
# Install dependencies
uv sync --extra dev

# Generate gRPC stubs
just gen-proto

# Run linting and type checking
just lint
just typecheck

# Run unit tests (fast, no Docker required)
just test

# Run integration tests (requires Docker)
just test-integration

# Run full CI suite
just ci
```

### Running the System

**Unified Development Mode (Recommended):**

```bash
# Start all services with Piper CPU TTS
just dev-agent-piper

# Start all services with CosyVoice GPU TTS
just dev cosyvoice2

# Access web client
open https://localhost:8443
```

**Docker Compose:**

```bash
# Start full Docker stack
docker compose up --build

# Start with CosyVoice GPU TTS
docker compose --profile cosyvoice up

# Access web client
open https://localhost:8443
```

**Individual Services (for debugging):**

```bash
# Start infrastructure
just redis          # Terminal 1: Redis

# Start TTS worker
just run-tts-piper  # Terminal 2: Piper TTS (CPU)
# OR
just run-tts-cosyvoice2  # Terminal 2: CosyVoice (GPU)

# Start orchestrator
just run-orch       # Terminal 3: Orchestrator (LiveKit Agent)

# Test with CLI client
just cli            # Terminal 4: CLI Client
```

### Testing & Validation

```bash
# Run specific integration test
uv run pytest tests/integration/test_m1_worker_integration.py -v

# Run VAD tests
uv run pytest tests/unit/test_vad.py -v
uv run pytest tests/integration/test_vad_integration.py -v

# Run with verbose gRPC logging
export GRPC_VERBOSITY=debug
just run-tts-mock

# Profile CPU usage
just spy-top <PID>

# Check logs
just logs-tail
just logs-list
```

---

## Performance Summary

### Latency Targets

| Metric | Target (CPU) | Target (GPU) | Target (GPU + Parallel) | Status |
|--------|--------------|--------------|-------------------------|--------|
| **Barge-in pause** | <50ms | <50ms | <50ms | âœ… Validated |
| **First Audio Latency** | <500ms | <300ms | <300ms | âœ… Validated |
| **ASR Transcription** | <3s | <1s | <1s | âœ… Validated |
| **Synthesis (per sentence)** | ~1.5s | ~1.5s | ~0.75s | âœ… Validated |
| **Total Response (5 sentences)** | ~7.5s | ~7.5s | ~3.5s | âœ… Validated |

### Throughput Metrics

**Sequential Mode (Baseline):**
- Sentences per second: 0.67 SPS
- Worker utilization: ~45%
- VRAM usage: 2-3 GB (GPU TTS)

**Parallel Mode (2 Workers):**
- Sentences per second: 1.33 SPS (2x improvement)
- Worker utilization: ~85%
- VRAM usage: 4-5 GB (GPU TTS)

**Parallel Mode (3 Workers):**
- Sentences per second: 2.00 SPS (3x improvement)
- Worker utilization: ~90%
- VRAM usage: 6-8 GB (GPU TTS)

### Real-World Performance

**User Testing (Commit 147e45c):**
- Before (Sequential): 9s total latency (1s ASR + 2s LLM + 6s TTS)
- After (Parallel + Warm-up): 6s total latency (1s ASR + 2s LLM + 3s TTS)
- **Improvement**: 33% reduction in total latency

---

## Documentation Index

### User Documentation

- **[QUICK_START.md](QUICK_START.md)**: Complete quick start guide (NEW - 2025-10-26)
- **[USER_GUIDE.md](USER_GUIDE.md)**: Comprehensive user journey (NEW - 2025-10-26)
- **[PARALLEL_TTS.md](PARALLEL_TTS.md)**: Parallel synthesis guide (NEW - 2025-10-26)
- **[CONFIGURATION.md](CONFIGURATION.md)**: Configuration reference (UPDATED - 2025-10-26)

### Technical Documentation

- **[CLAUDE.md](../CLAUDE.md)**: Project overview and development guide
- **[docs/CURRENT_STATUS.md](CURRENT_STATUS.md)**: This file - current implementation status
- **[docs/TESTING_GUIDE.md](TESTING_GUIDE.md)**: Testing strategy, commands, and troubleshooting
- **[docs/PERFORMANCE.md](PERFORMANCE.md)**: Performance benchmarks and targets

### Technical Design

- **[project_documentation/PRD.md](../project_documentation/PRD.md)**: Product requirements
- **[project_documentation/TDD.md](../project_documentation/TDD.md)**: Technical design document
- **[project_documentation/INCREMENTAL_IMPLEMENTATION_PLAN.md](../project_documentation/INCREMENTAL_IMPLEMENTATION_PLAN.md)**: Milestone breakdown

### Operational Guides

- **[GRPC_SEGFAULT_WORKAROUND.md](archive/working/GRPC_SEGFAULT_WORKAROUND.md)**: WSL2 gRPC testing workaround (exemplary)
- **[DOCKER_UNIFIED_WORKFLOW.md](DOCKER_UNIFIED_WORKFLOW.md)**: Unified development workflow
- **[VOICEPACK_COSYVOICE2.md](VOICEPACK_COSYVOICE2.md)**: CosyVoice voicepack setup

---

## Success Metrics

### M0-M10 + Parallel Synthesis Achievements âœ…

- âœ… 790 total tests passing in CI (100% pass rate)
- âœ… <50ms control command response time
- âœ… <50ms VAD barge-in pause latency (p95)
- âœ… <5ms VAD processing latency per frame
- âœ… <300ms First Audio Latency (GPU TTS)
- âœ… <500ms First Audio Latency (CPU TTS)
- âœ… <1s ASR transcription latency (GPU)
- âœ… <3s ASR transcription latency (CPU)
- âœ… 2x synthesis throughput with parallel mode
- âœ… 50% latency reduction for multi-sentence responses
- âœ… 85% sustained worker utilization (parallel mode)
- âœ… Persistent worker pool (eliminates cold-start)
- âœ… FIFO ordering guarantee in parallel mode
- âœ… TTL-based eviction working (configurable timeout)
- âœ… LRU eviction when capacity exceeded
- âœ… Reference counting prevents in-use model unload
- âœ… LiveKit WebRTC primary transport operational
- âœ… Docker compose stack with 5 services
- âœ… gRPC segfault workaround 100% reliable
- âœ… CI passing (lint + typecheck + test)
- âœ… Two real TTS adapters implemented (Piper CPU, CosyVoice GPU)
- âœ… Shared audio utilities extracted (resampling, framing)
- âœ… AdapterState enum unified across adapters
- âœ… ModelManager prefix routing (piper-*, cosyvoice2-*)
- âœ… PyTorch version conflict documented and solution designed
- âœ… Voicepack structure for CosyVoice 2 models
- âœ… User validation: "pretty good!" feedback

---

## Contributing

### Before Starting Work

1. Read [CLAUDE.md](../CLAUDE.md) for project overview
2. Review this file for current status
3. Check [docs/TESTING_GUIDE.md](TESTING_GUIDE.md) for test strategy
4. Run `just ci` to validate local environment

### Development Checklist

```markdown
- [ ] Update CURRENT_STATUS.md if milestone progress
- [ ] Update CLAUDE.md if architecture changes
- [ ] Add tests for new features
- [ ] Run `just ci` before committing
- [ ] Update "Last Updated" dates in docs
```

### Getting Help

- **Documentation Issues**: See [PHASE1_REPORT_DOCUMENTATION_AUDIT.md](../PHASE1_REPORT_DOCUMENTATION_AUDIT.md)
- **Test Issues**: See [docs/TESTING_GUIDE.md](TESTING_GUIDE.md)
- **gRPC/WSL2 Issues**: See [GRPC_SEGFAULT_WORKAROUND.md](archive/working/GRPC_SEGFAULT_WORKAROUND.md)

---

**Maintained by**: Documentation Engineering Team
**Last Review**: 2025-10-26
**Next Review**: After M7 completion
