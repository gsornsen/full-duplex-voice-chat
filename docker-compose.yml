services:
  # Redis for service discovery
  redis:
    image: redis:7-alpine
    container_name: redis-tts
    # No external port mapping - only accessible within Docker network
    # This prevents port conflicts with host Redis/Valkey instances
    # Services use internal DNS: redis:6379
    # To access from host for debugging: docker compose exec redis redis-cli
    healthcheck:
      test: ["CMD", "redis-cli", "-h", "localhost", "-p", "6379", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
      start_period: 5s
    networks:
      - tts-network

  # LiveKit server for WebRTC transport
  livekit:
    image: livekit/livekit-server:latest
    container_name: livekit-server
    command: --config /etc/livekit.yaml
    ports:
      - "7880:7880"   # WebRTC/WebSocket
      - "7881:7881"   # RTC TCP port
      - "7882:7882/udp" # TURN/UDP
      - "50000-50099:50000-50099/udp"  # RTC port range (subset for Docker)
    volumes:
      - ./configs/livekit.yaml:/etc/livekit.yaml:ro
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:7880/"]
      interval: 10s
      timeout: 3s
      retries: 5
      start_period: 30s
    networks:
      - tts-network

  # Caddy reverse proxy for HTTPS
  caddy:
    image: caddy:2-alpine
    container_name: caddy-proxy
    ports:
      - "8443:8443"    # HTTPS for web client
      - "8444:8444"    # HTTPS for LiveKit WebSocket
      - "80:80"        # HTTP (optional, for redirects)
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile:ro
      - ./voicechat.local+3.pem:/etc/caddy/voicechat.local+3.pem:ro
      - ./voicechat.local+3-key.pem:/etc/caddy/voicechat.local+3-key.pem:ro
      - caddy-data:/data
      - caddy-config:/config
      - caddy-logs:/var/log/caddy
    depends_on:
      livekit:
        condition: service_healthy
    extra_hosts:
      # Allow Caddy to reach host machine (for Next.js on host:3000)
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:2019/config/"]
      interval: 10s
      timeout: 3s
      retries: 5
      start_period: 5s
    networks:
      - tts-network
    restart: unless-stopped

  # Orchestrator service - supports both LiveKit Agent and Legacy WebSocket modes
  # Mode selection via ORCHESTRATOR_MODE environment variable:
  #   - agent (default): LiveKit Agent mode for web frontend
  #   - legacy: WebSocket server mode for CLI client
  orchestrator:
    build:
      context: .
      dockerfile: Dockerfile.orchestrator
    container_name: orchestrator
    # Use entrypoint script to select mode based on ORCHESTRATOR_MODE
    entrypoint: ["/app/docker/entrypoint-orchestrator.sh"]
    # Graceful shutdown for LiveKit Agent worker deregistration
    stop_signal: SIGTERM
    stop_grace_period: 90s  # Increased from 30s to match shutdown_process_timeout
    ports:
      - "8080:8080"  # WebSocket (legacy mode only)
      - "8081:8081"  # Health check HTTP (legacy mode only)
    environment:
      # Orchestrator mode: agent (default) or legacy
      - ORCHESTRATOR_MODE=${ORCHESTRATOR_MODE:-agent}
      # Use internal Docker DNS and correct Redis port (6379 inside container)
      - REDIS_URL=redis://redis:6379
      - LIVEKIT_URL=ws://livekit:7880
      - LIVEKIT_API_KEY=devkey
      - LIVEKIT_API_SECRET=devsecret1234567890abcdefghijklmn
      # ASR Configuration (WhisperX)
      - ASR_ENABLED=true
      - ASR_ADAPTER=whisperx
      - ASR_DEVICE=${ASR_DEVICE:-auto}
      - ASR_MODEL_SIZE=${ASR_MODEL_SIZE:-small}
      - ASR_LANGUAGE=${ASR_LANGUAGE:-en}
      - ASR_COMPUTE_TYPE=${ASR_COMPUTE_TYPE:-default}
      # TTS Configuration
      - TTS_WORKER_ADDRESS=tts:7001
      - ADAPTER_TYPE=${ADAPTER_TYPE:-piper}
      - DEFAULT_MODEL=${DEFAULT_MODEL:-piper-en-us-lessac-medium}
      - DEFAULT_MODEL_ID=${DEFAULT_MODEL_ID:-piper-en-us-lessac-medium}
      # Required for agent mode (OpenAI LLM integration)
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
    depends_on:
      redis:
        condition: service_healthy
      livekit:
        condition: service_healthy
      # TTS worker dependency removed - orchestrator connects via network alias
      # Either tts0 (piper profile) or tts-cosyvoice (cosyvoice profile) will provide the "tts" alias
    healthcheck:
      # Health check only works in legacy mode (agent mode doesn't expose HTTP endpoint)
      test: ["CMD-SHELL", "curl -f http://localhost:8081/health || exit 0"]
      interval: 10s
      timeout: 5s
      retries: 3
      # Increased from 10s to 150s to account for:
      # - WhisperX model download (if not cached): ~10-15s
      # - WhisperX model loading to GPU: ~15-20s
      # - Pre-warming initialization: ~5s
      # - Worker registration: ~5s
      # - Buffer time for slow systems/network: ~95s
      start_period: 150s
    networks:
      - tts-network
    volumes:
      # Mount entrypoint script
      - ./docker/entrypoint-orchestrator.sh:/app/docker/entrypoint-orchestrator.sh:ro
      # Mount voicepacks for config validation (read-only)
      - ./voicepacks:/app/voicepacks:ro
      # Persistent model caches (eliminates runtime downloads)
      - huggingface-cache:/root/.cache/huggingface
      - torch-cache:/root/.cache/torch
      - mfa-cache:/root/.cache/mfa
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # count: 1
              capabilities: [gpu]

  # TTS Worker 0 (Default: Piper CPU)
  # Uses environment variables: ADAPTER_TYPE, DEFAULT_MODEL
  # For CosyVoice, use the cosyvoice profile instead (docker compose --profile cosyvoice up)
  tts0:
    profiles:
      - piper  # Only start with --profile piper (prevents conflict with cosyvoice profile)
    build:
      context: .
      dockerfile: Dockerfile.tts
    container_name: tts-worker-0
    ports:
      - "7001:7001"  # gRPC (changed from 7002 to match config)
      - "9090:9090"  # Metrics
    environment:
      # Use internal Docker DNS and correct Redis port (6379 inside container)
      - REDIS_URL=redis://redis:6379
      - CUDA_VISIBLE_DEVICES=0
      - WORKER_NAME=tts-worker-0
      # TTS Configuration - ensure model matches deployment profile
      - ADAPTER_TYPE=${ADAPTER_TYPE:-piper}
      - DEFAULT_MODEL=${DEFAULT_MODEL:-piper-en-us-lessac-medium}
      - DEFAULT_MODEL_ID=${DEFAULT_MODEL_ID:-piper-en-us-lessac-medium}
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      # Use bash built-in TCP test instead of nc
      test: ["CMD-SHELL", "timeout 1 bash -c '</dev/tcp/localhost/7001' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s
    networks:
      tts-network:
        aliases:
          - tts  # Common alias for orchestrator to connect regardless of model
    volumes:
      - ./voicepacks:/app/voicepacks:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # device_ids: ['0']
              capabilities: [gpu]

  # TTS Worker (CosyVoice GPU)
  # Activated with: docker compose --profile cosyvoice up
  # Uses isolated PyTorch 2.3.1 environment (see docs/COSYVOICE_PYTORCH_CONFLICT.md)
  # This service uses a separate Dockerfile to avoid PyTorch version conflicts
  # with the main project (PyTorch 2.7.0 vs CosyVoice requirement of 2.3.1)
  tts-cosyvoice:
    build:
      context: .
      dockerfile: Dockerfile.tts-cosyvoice
    container_name: tts-cosyvoice
    ports:
      - "7002:7001"  # gRPC (expose on host 7002, container uses 7001)
      - "9091:9091"  # Metrics
    environment:
      - REDIS_URL=redis://redis:6379
      - CUDA_VISIBLE_DEVICES=0
      - WORKER_NAME=tts-cosyvoice
      - WORKER_PORT=7001  # Use same internal port as tts0 for network alias
      # TTS Configuration - CosyVoice specific
      - ADAPTER_TYPE=cosyvoice2
      - DEFAULT_MODEL=cosyvoice2-en-base
      - DEFAULT_MODEL_ID=cosyvoice2-en-base
      - TTL_MS=600000
      - RESIDENT_CAP=2
      - LOG_LEVEL=INFO
    env_file:
      - .env.cosyvoice  # Optional: override with custom configuration
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "timeout 1 bash -c '</dev/tcp/localhost/7001' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 90s  # CosyVoice model loading takes longer
    networks:
      tts-network:
        aliases:
          - tts  # Common alias for orchestrator to connect regardless of model
    volumes:
      # Mount voicepacks directory for CosyVoice models (rw for auto-download)
      - ./voicepacks/cosyvoice:/app/voicepacks/cosyvoice
      # Optional: Mount CosyVoice repository if needed
      # - ./third_party/CosyVoice:/app/CosyVoice:ro
      # Persistent model caches (eliminates runtime downloads)
      - modelscope-cache:/root/.cache/modelscope  # WeText FST files
      - huggingface-cache:/root/.cache/huggingface  # Shared with orchestrator
      - torch-cache:/root/.cache/torch  # Shared with orchestrator
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
        limits:
          memory: 8G  # CosyVoice 2 needs ~6-8GB VRAM
    profiles:
      - cosyvoice  # Only start with: docker compose --profile cosyvoice up

networks:
  tts-network:
    driver: bridge

volumes:
  caddy-data:
    driver: local
  caddy-config:
    driver: local
  caddy-logs:
    driver: local
  # Model cache volumes (persistent across container rebuilds)
  # Eliminates runtime downloads of WhisperX, WeText, and PyTorch models
  huggingface-cache:
    driver: local
  torch-cache:
    driver: local
  mfa-cache:
    driver: local
  modelscope-cache:
    driver: local
